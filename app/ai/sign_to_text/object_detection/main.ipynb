{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "import paths\n",
    "\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels that our model will predict\n",
    "labels = [\n",
    "    {\n",
    "        'name':'K', \n",
    "        'id':1\n",
    "    },\n",
    "    {\n",
    "        'name':'O', \n",
    "        'id':2\n",
    "    },\n",
    "    {\n",
    "        'name':'V', \n",
    "        'id':3\n",
    "    },\n",
    "]\n",
    "\n",
    "with open(paths.ANNOTATION_PATH + '\\label_map.pbtxt', 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating TF records for our model to be able to train\n",
    "!python {paths.SCRIPTS_PATH + '/generate_tfrecord.py'} -x {paths.IMAGE_PATH + '/train'} -l {paths.ANNOTATION_PATH + '/label_map.pbtxt'} -o {paths.ANNOTATION_PATH + '/train.record'}\n",
    "!python {paths.SCRIPTS_PATH + '/generate_tfrecord.py'} -x{paths.IMAGE_PATH + '/test'} -l {paths.ANNOTATION_PATH + '/label_map.pbtxt'} -o {paths.ANNOTATION_PATH + '/test.record'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the destination directory if it does not exist\n",
    "dest_dir = os.path.join(paths.MODEL_PATH, paths.CUSTOM_MODEL_NAME)\n",
    "if not os.path.exists(dest_dir):\n",
    "  os.makedirs(dest_dir)\n",
    "  print(f\"Directory created: {dest_dir}\")\n",
    "else:\n",
    "  print(f\"Directory already exists: {dest_dir}\")\n",
    "\n",
    "# Define the source and destination paths for the config file\n",
    "source_config = os.path.join(paths.PRETRAINED_MODEL_PATH,\n",
    "  'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n",
    "  'pipeline.config')\n",
    "dest_config = os.path.join(dest_dir, 'pipeline.config')\n",
    "\n",
    "# Copy config from source to destination\n",
    "try:\n",
    "  shutil.copy(source_config, dest_config)\n",
    "  print(f\"Copied '{source_config}' to '{dest_config}'\")\n",
    "except Exception as e:\n",
    "  print(f\"Error copying file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_util.get_configs_from_pipeline_file(paths.CONFIG_PATH)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(paths.CONFIG_PATH, \"r\") as f:\n",
    "  proto_str = f.read()\n",
    "  text_format.Merge(proto_str, pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup config for the model\n",
    "pipeline_config.model.ssd.num_classes = 3\n",
    "pipeline_config.train_config.batch_size = 4\n",
    "pipeline_config.train_config.fine_tune_checkpoint = paths.PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0'\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_input_reader.label_map_path= paths.ANNOTATION_PATH + '/label_map.pbtxt'\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [paths.ANNOTATION_PATH + '/train.record']\n",
    "pipeline_config.eval_input_reader[0].label_map_path = paths.ANNOTATION_PATH + '/label_map.pbtxt'\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [paths.ANNOTATION_PATH + '/test.record']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save new config\n",
    "config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n",
    "with tf.io.gfile.GFile(paths.CONFIG_PATH, \"wb\") as f:                                                                                                                                                                                                                     \n",
    "    f.write(config_text)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get command for model training\n",
    "print(f\"python {paths.APIMODEL_PATH}/research/object_detection/model_main_tf2.py --model_dir={paths.MODEL_PATH}/{paths.CUSTOM_MODEL_NAME} --pipeline_config_path={paths.MODEL_PATH}/{paths.CUSTOM_MODEL_NAME}/pipeline.config --num_train_steps=100000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get evaluation command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"python {paths.APIMODEL_PATH}/research/object_detection/model_main_tf2.py --model_dir={paths.MODEL_PATH}/{paths.CUSTOM_MODEL_NAME} --pipeline_config_path={paths.MODEL_PATH}/{paths.CUSTOM_MODEL_NAME}/pipeline.config --checkpoint_dir={paths.MODEL_PATH}/{paths.CUSTOM_MODEL_NAME}\")\n",
    "\n",
    "# --model_dir + train ili eval folder u tom folderu mogu runnati tensorboard --logdir=. i dobiti performanse modela"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obj_detection_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
